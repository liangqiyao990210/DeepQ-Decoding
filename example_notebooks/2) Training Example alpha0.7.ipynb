{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Training Decoders in Practice\n",
    "\n",
    "Now that we have discussed the conceptual foundations, strategies and techniques involved, we will provide detailed examples of how train decoders via the procedures discussed. In particular, in this notebook we will walk through a very simple script for training a decoder with a given set of hyper-parameters, providing the foundation for a later discussion concerning how to obtain optimal decoders for a range of error rates through an iterative training procedure involving a hyper-parameter optimization for each error rate (see the companion notebook \"Large Scale Iterative Training\"). \n",
    "\n",
    "##### 2a) Requirements\n",
    "\n",
    "The following packages are required, and can be installed via PIP:\n",
    "\n",
    "<ol>\n",
    "  <li> Python 3 (with numpy and scipy)</li>\n",
    "  <li> tensorflow </li>\n",
    "  <li> keras </li> \n",
    "  <li> gym </li> \n",
    "</ol> \n",
    "\n",
    "In addition, a modified version of the Keras-RL package is required, which should be installed from <a href=\"https://github.com/R-Sweke/keras-rl\">this fork</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2b) A Simple Training Script\n",
    "\n",
    "We begin by importing all required packages and methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\Users\\Edward\\anaconda3\\envs\\threepointsix\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\Users\\Edward\\anaconda3\\envs\\threepointsix\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\Users\\Edward\\anaconda3\\envs\\threepointsix\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\Users\\Edward\\anaconda3\\envs\\threepointsix\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\Users\\Edward\\anaconda3\\envs\\threepointsix\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\Users\\Edward\\anaconda3\\envs\\threepointsix\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\Users\\Edward\\anaconda3\\envs\\threepointsix\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\Users\\Edward\\anaconda3\\envs\\threepointsix\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\Users\\Edward\\anaconda3\\envs\\threepointsix\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\Users\\Edward\\anaconda3\\envs\\threepointsix\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\Users\\Edward\\anaconda3\\envs\\threepointsix\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\Users\\Edward\\anaconda3\\envs\\threepointsix\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\Users\\Edward\\anaconda3\\envs\\threepointsix\\lib\\site-packages\\gym\\core.py:27: UserWarning: \u001b[33mWARN: Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\u001b[0m\n",
      "  \"Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow\n",
    "import gym\n",
    "\n",
    "from Function_Library import *\n",
    "from Environments import *\n",
    "\n",
    "import rl as rl\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy, LinearAnnealedPolicy, GreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.callbacks import FileLogger\n",
    "\n",
    "import json\n",
    "import copy\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then proceed by providing all required hyperparameters and physical configuration settings. In order to allow for easier grid searching and incremented training later on we choose to split all hyperparameters into two categories:\n",
    "\n",
    "   - fixed configs: These remain constant during the course of a grid search or incremented training procedure.\n",
    "   - variable configs: We will later set up training grids over these hyperparameters.\n",
    "    \n",
    "In particular, the fixed parameters one must provide are:\n",
    "\n",
    "   1. **d**: The lattice width (equal to the lattice height)\n",
    "   - **use_Y**: If true then the agent can perform Y Pauli flips directly, if False then the agent can only perform X and Z Pauli flips.\n",
    "   - **train_freq**: The number of agent-environment interaction steps which occur between each updating of the agent's weights.\n",
    "   - **batch_size**: The size of batches used for calculating loss functions for gradient descent updates of agent weights.\n",
    "   - **print_freq**: Every print_freq episodes the statistics of the training procedure will be logged.\n",
    "   - **rolling_average_length**: The number of most recent episodes over which any relevant rolling average will be calculated.\n",
    "   - **stopping_patience**: The number of episodes after which no improvement will result in the early stopping of the training procedure.\n",
    "   - **error_model**: A string in [\"X\", \"DP\"], specifiying the noise model of the environment as X flips only or depolarizing noise.\n",
    "   - **c_layers**: A list of lists specifying the structure of the convolutional layers of the agent deepQ network. Each inner list describes a layer and has the form [num_filters, filter_width, stride].\n",
    "   - **ff_layers**: A list of lists specifying the structure of the feed-forward neural network sitting on top of the convolutional neural network. Each inner list has the form [num_neurons, output_dropout_rate].\n",
    "   - **max_timesteps**: The maximum number of training timesteps allowed.\n",
    "   - **volume_depth**: The number of syndrome measurements taken each time a new syndrome extraction is performed - i.e. the depth of the syndrome volume passed to the agent.\n",
    "   - **testing_length**: The number of episodes uses to evaluate the trained agents performance. \n",
    "   - **buffer_size**: The maximum number of experience tuples held in the memory from which the update batches for agent updating are drawn.\n",
    "   - **dueling**: A boolean indicating whether or not a [dueling architecture](https://arxiv.org/abs/1511.06581) should be used.\n",
    "   - **masked_greedy**: A boolean which indicates whether the agent will only be allowed to choose legal actions (actions next to an anyon or previously flipped qubit) when acting greedily (i.e. when choosing actions via the argmax of the Q-values)\n",
    "   - **static_decoder**: For training within the fault tolerant setting (multi-cycle decoding) this should always be set to True.\n",
    "   \n",
    "In addition, the parameters which we will later incrementally vary or grid search around are:\n",
    "\n",
    "   1. **p_phys**: The physical error probability\n",
    "   2. **p_meas**: The measurement error probability\n",
    "   3. **success_threshold**: The qubit lifetime rolling average at which training has been deemed succesfull and will be stopped.\n",
    "   4. **learning_starts**: The number of initial steps taken to contribute experience tuples to memory before any weight updates are made.\n",
    "   5. **learning_rate**: The learning rate for gradient descent optimization (via the Adam optimizer)\n",
    "   6. **exploration_fraction**: The number of time steps over which epsilon, the parameter controlling the probability of a random explorative action, is annealed.\n",
    "   7. **max_eps**: The initial maximum value of epsilon.\n",
    "   8. **target_network_update_freq**: In order to achieve stable training, a target network is cloned off from the active deepQ agent every target_network_update_freq interval of steps. This target network is then used to generate the target Q-function over the following interval.\n",
    "   9. **gamma**: The discount rate used for calculating the expected discounted cumulative return (the Q-values).\n",
    "   10. **final_eps**: The final value at which annealing of epsilon will be stopped.\n",
    "   \n",
    "Furthermore, in addition to all the above parameters one must provide a directory into which results and training progress as logged, as well as the path to a pre-trained referee decoder. Here e provide two pre-trained feed forward classification based referee decoders, one for X noise and one for DP noise. However, in principle any perfect-measurement decoding algorithm (such as MWPM) could be used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[548 714 602 544 423 645 437 890 962 383 791 528 567 924  71  87  21 831\n",
      " 777 869 977 798 461 779 119 639 144 943 521 414]\n",
      "[548 714 602 544 423 645 437 890 962 383 791 528 567 924  71  87  21 831\n",
      " 777 869 977 798 461 779 119 639 144 943 521 414]\n",
      "[993 995 994 993 990 994 990 997 998 989 996 992 993 998 973 975 961 997\n",
      " 996 997 998 996 991 996 978 994 979 998 992 990]\n",
      "[ 8 13 10  8  6 11  6 22 33  5 16  8  9 26  1  1  1 18 15 20 38 16  7 15\n",
      "  2 11  2 29  8  6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from rl.memory import sample_batch_indexes\n",
    "\n",
    "LENGTH = 1000\n",
    "LOW_INDEX = 1\n",
    "HIGH_INDEX = LENGTH - 1\n",
    "SAMPLE_SIZE = 30\n",
    "\n",
    "np.random.seed(0)\n",
    "print(sample_batch_indexes(LOW_INDEX, HIGH_INDEX, SAMPLE_SIZE, priorities=np.ones(LENGTH)))\n",
    "np.random.seed(0)\n",
    "print(sample_batch_indexes(LOW_INDEX, HIGH_INDEX, SAMPLE_SIZE, priorities=np.array(range(LENGTH))))\n",
    "np.random.seed(0)\n",
    "print(sample_batch_indexes(LOW_INDEX, HIGH_INDEX, SAMPLE_SIZE, priorities=np.array(range(LENGTH)), alpha=100.0))\n",
    "np.random.seed(0)\n",
    "print(sample_batch_indexes(LOW_INDEX, HIGH_INDEX, SAMPLE_SIZE, priorities=np.array(list(reversed(range(LENGTH)))), alpha=100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.memory import RingBuffer\n",
    "\n",
    "hi = RingBuffer(10)\n",
    "for i in range(30):\n",
    "    hi.append(i)\n",
    "\n",
    "assert hi.get_max() == 29\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 100.001, 1, 200.001]\n",
      "observations: [17, 18, 19, 20, 21, 22, 2, 2, 15, 16]\n",
      "End state of priorities: [1, 1, 1, 1, 1, 1, 1000.001, 200.001, 300.001, 200.001]\n"
     ]
    }
   ],
   "source": [
    "from rl.memory import SequentialMemory\n",
    "\n",
    "MEMORY_LENGTH = 10\n",
    "new_mem_who_dis = SequentialMemory(MEMORY_LENGTH, window_length=1, enable_prioritized_replay=True)\n",
    "for i in range(3):\n",
    "    new_mem_who_dis.append(i, i, i, i)\n",
    "\n",
    "assert new_mem_who_dis.nb_entries == 3, \"Memory should contain 3 items so far.\"\n",
    "\n",
    "for i in range(MEMORY_LENGTH * 2 + 3):\n",
    "    new_mem_who_dis.append(i, i, i, i)\n",
    "\n",
    "assert len(new_mem_who_dis.priorities) == MEMORY_LENGTH, \"Memory should now be capped at the memory length\"\n",
    "eps = 0.001\n",
    "new_mem_who_dis.update_priorities([1, 3], [100, 200], eps=eps)\n",
    "\n",
    "print(new_mem_who_dis.priorities.data)\n",
    "\n",
    "assert new_mem_who_dis.priorities.get_max() == 200 + eps\n",
    "assert new_mem_who_dis.priorities[3] == 200 + eps\n",
    "\n",
    "new_mem_who_dis.append(2, 2, 2, 2)\n",
    "new_mem_who_dis.append(2, 2, 2, 2)\n",
    "\n",
    "print(\"observations:\", new_mem_who_dis.observations.data)\n",
    "\n",
    "assert new_mem_who_dis.priorities[MEMORY_LENGTH - 1] == 200 + eps, \"Newest item added should be the max after append is called.\"\n",
    "\n",
    "new_mem_who_dis.update_priorities([0, 8], [300, 1000], eps=eps)\n",
    "\n",
    "print(\"End state of priorities:\", new_mem_who_dis.priorities.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_configs = {\"d\": 5,\n",
    "                \"use_Y\": False,\n",
    "                \"train_freq\": 1,\n",
    "                \"batch_size\": 32,\n",
    "                \"print_freq\": 250,\n",
    "                \"rolling_average_length\": 500,\n",
    "                \"stopping_patience\": 500,\n",
    "                \"error_model\": \"X\",\n",
    "                \"c_layers\": [[64,3,2],[32,2,1],[32,2,1]],\n",
    "                \"ff_layers\": [[512,0.2]],\n",
    "                \"max_timesteps\": 1000000,\n",
    "                \"volume_depth\": 5,\n",
    "                \"testing_length\": 101,\n",
    "                \"buffer_size\": 50000,\n",
    "                \"dueling\": True,\n",
    "                \"masked_greedy\": False,\n",
    "                \"static_decoder\": True}\n",
    "\n",
    "variable_configs = {\"p_phys\": 0.001,\n",
    "                    \"p_meas\": 0.001,\n",
    "                    \"success_threshold\": 10000,\n",
    "                    \"learning_starts\": 1000,\n",
    "                    \"learning_rate\": 0.00001,\n",
    "                    \"exploration_fraction\": 100000,\n",
    "                    \"max_eps\": 1.0,\n",
    "                    \"target_network_update_freq\": 5000,\n",
    "                    \"gamma\": 0.99,\n",
    "                    \"alpha\": 0.7,\n",
    "                    \"final_eps\": 0.02}\n",
    "\n",
    "logging_directory = os.path.join(os.getcwd(),\"logging_directory/\")\n",
    "static_decoder_path = os.path.join(os.getcwd(),\"referee_decoders/nn_d5_X_p5\")\n",
    "\n",
    "\n",
    "all_configs = {}\n",
    "\n",
    "for key in fixed_configs.keys():\n",
    "    all_configs[key] = fixed_configs[key]\n",
    "\n",
    "for key in variable_configs.keys():\n",
    "    all_configs[key] = variable_configs[key]\n",
    "\n",
    "static_decoder = load_model(static_decoder_path)                                                 \n",
    "logging_path = os.path.join(logging_directory,\"training_history.json\")\n",
    "logging_callback = FileLogger(filepath = logging_path,interval = all_configs[\"print_freq\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have specified all the required parameters we can instantiate our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Surface_Code_Environment_Multi_Decoding_Cycles(d=all_configs[\"d\"], \n",
    "    p_phys=all_configs[\"p_phys\"], \n",
    "    p_meas=all_configs[\"p_meas\"],  \n",
    "    error_model=all_configs[\"error_model\"], \n",
    "    use_Y=all_configs[\"use_Y\"], \n",
    "    volume_depth=all_configs[\"volume_depth\"],\n",
    "    static_decoder=static_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment class is defined to mirror the environments of [https://gym.openai.com/](openAI gym), and such contains the required \"reset\" and \"step\" methods, via which the agent can interact with the environment, in addition to decoding specific methods and attributes whose details can be found in the relevant method docstrings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now proceed to define the agent. We being by specifying the memory to be used, as well as the exploration and testing policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE_PRIORITIZED_REPLAY = True\n",
    "memory = SequentialMemory(limit=all_configs[\"buffer_size\"], window_length=1, enable_prioritized_replay=ENABLE_PRIORITIZED_REPLAY)\n",
    "\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(masked_greedy=all_configs[\"masked_greedy\"]), \n",
    "    attr='eps', value_max=all_configs[\"max_eps\"], \n",
    "    value_min=all_configs[\"final_eps\"], \n",
    "    value_test=0.0, \n",
    "    nb_steps=all_configs[\"exploration_fraction\"])\n",
    "\n",
    "test_policy = GreedyQPolicy(masked_greedy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can then build the deep convolutional neural network which will represent our Q-function and compile our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_convolutional_nn(all_configs[\"c_layers\"], \n",
    "                               all_configs[\"ff_layers\"], \n",
    "                               env.observation_space.shape, \n",
    "                               env.num_actions)\n",
    "\n",
    "dqn = DQNAgent(model=model, \n",
    "               nb_actions=env.num_actions, \n",
    "               memory=memory, \n",
    "               nb_steps_warmup=all_configs[\"learning_starts\"], \n",
    "               target_model_update=all_configs[\"target_network_update_freq\"], \n",
    "               policy=policy,\n",
    "               test_policy = test_policy,\n",
    "               gamma = all_configs[\"gamma\"],\n",
    "               enable_dueling_network=all_configs[\"dueling\"],\n",
    "               enable_prioritized_replay=ENABLE_PRIORITIZED_REPLAY,\n",
    "               alpha=all_configs[\"alpha\"])  \n",
    "\n",
    "\n",
    "dqn.compile(Adam(lr=all_configs[\"learning_rate\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With both the agent and the environment specified, it is then possible to train the agent by calling the agent's \"fit\" method. If you want to run this on a single computer, be careful, it may take up to 12 hours!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000000 steps ...\n",
      "-----------------\n",
      "                \n",
      "Episode: 250\n",
      "Step: 2321/1000000\n",
      "This Episode Steps: 1\n",
      "This Episode Reward: 0.0\n",
      "This Episode Duration: 0.025s\n",
      "Rolling Lifetime length: 40.960\n",
      "Best Lifetime Rolling Avg: 47.8125\n",
      "Best Episode: 15\n",
      "Time Since Best: 234\n",
      "Has Succeeded: False\n",
      "Stopped Improving: False\n",
      "Metrics: loss: 0.076101, mean_q: 0.234859, mean_eps: 0.977264\n",
      "Total Training Time: 22.520s\n",
      "\n",
      "-----------------\n",
      "                \n",
      "Episode: 500\n",
      "Step: 4686/1000000\n",
      "This Episode Steps: 6\n",
      "This Episode Reward: 1.0\n",
      "This Episode Duration: 0.119s\n",
      "Rolling Lifetime length: 40.430\n",
      "Best Lifetime Rolling Avg: 47.8125\n",
      "Best Episode: 15\n",
      "Time Since Best: 484\n",
      "Has Succeeded: False\n",
      "Stopped Improving: False\n",
      "Metrics: loss: 0.057208, mean_q: 0.280417, mean_eps: 0.954112\n",
      "Total Training Time: 62.079s\n",
      "\n",
      "-----------------\n",
      "                \n",
      "Episode: 750\n",
      "Step: 7259/1000000\n",
      "This Episode Steps: 15\n",
      "This Episode Reward: 0.0\n",
      "This Episode Duration: 0.357s\n",
      "Rolling Lifetime length: 39.360\n",
      "Best Lifetime Rolling Avg: 47.8125\n",
      "Best Episode: 15\n",
      "Time Since Best: 734\n",
      "Has Succeeded: False\n",
      "Stopped Improving: False\n",
      "Metrics: loss: 0.053231, mean_q: 0.499483, mean_eps: 0.928940\n",
      "Total Training Time: 116.464s\n",
      "\n",
      "-----------------\n",
      "                \n",
      "Episode: 1000\n",
      "Step: 9718/1000000\n",
      "This Episode Steps: 2\n",
      "This Episode Reward: 0.0\n",
      "This Episode Duration: 0.078s\n",
      "Rolling Lifetime length: 38.080\n",
      "Best Lifetime Rolling Avg: 47.8125\n",
      "Best Episode: 15\n",
      "Time Since Best: 984\n",
      "Has Succeeded: False\n",
      "Stopped Improving: False\n",
      "Metrics: loss: 0.050348, mean_q: 0.489512, mean_eps: 0.904778\n",
      "Total Training Time: 180.139s\n",
      "\n",
      "-----------------\n",
      "                \n",
      "Episode: 1250\n",
      "Step: 12450/1000000\n",
      "This Episode Steps: 10\n",
      "This Episode Reward: 3.0\n",
      "This Episode Duration: 0.338s\n",
      "Rolling Lifetime length: 42.110\n",
      "Best Lifetime Rolling Avg: 47.8125\n",
      "Best Episode: 15\n",
      "Time Since Best: 1234\n",
      "Has Succeeded: False\n",
      "Stopped Improving: False\n",
      "Metrics: loss: 0.075979, mean_q: 0.790642, mean_eps: 0.878044\n",
      "Total Training Time: 264.492s\n",
      "\n",
      "-----------------\n",
      "                \n",
      "Episode: 1500\n",
      "Step: 14895/1000000\n",
      "This Episode Steps: 6\n",
      "This Episode Reward: 1.0\n",
      "This Episode Duration: 0.269s\n",
      "Rolling Lifetime length: 45.290\n",
      "Best Lifetime Rolling Avg: 47.8125\n",
      "Best Episode: 15\n",
      "Time Since Best: 1484\n",
      "Has Succeeded: False\n",
      "Stopped Improving: False\n",
      "Metrics: loss: 0.077996, mean_q: 0.786232, mean_eps: 0.854063\n",
      "Total Training Time: 354.362s\n",
      "\n",
      "-----------------\n",
      "                \n",
      "Episode: 1750\n",
      "Step: 17583/1000000\n",
      "This Episode Steps: 2\n",
      "This Episode Reward: 0.0\n",
      "This Episode Duration: 0.113s\n",
      "Rolling Lifetime length: 43.890\n",
      "Best Lifetime Rolling Avg: 47.8125\n",
      "Best Episode: 15\n",
      "Time Since Best: 1734\n",
      "Has Succeeded: False\n",
      "Stopped Improving: False\n",
      "Metrics: loss: 0.059012, mean_q: 0.938983, mean_eps: 0.827701\n",
      "Total Training Time: 464.316s\n",
      "\n",
      "-----------------\n",
      "                \n",
      "Episode: 2000\n",
      "Step: 20309/1000000\n",
      "This Episode Steps: 3\n",
      "This Episode Reward: 1.0\n",
      "This Episode Duration: 0.153s\n",
      "Rolling Lifetime length: 47.050\n",
      "Best Lifetime Rolling Avg: 47.8125\n",
      "Best Episode: 15\n",
      "Time Since Best: 1984\n",
      "Has Succeeded: False\n",
      "Stopped Improving: False\n",
      "Metrics: loss: 0.134307, mean_q: 1.158468, mean_eps: 0.800991\n",
      "Total Training Time: 595.720s\n",
      "\n",
      "-----------------\n",
      "                \n",
      "Episode: 2250\n",
      "Step: 23076/1000000\n",
      "This Episode Steps: 9\n",
      "This Episode Reward: 0.0\n",
      "This Episode Duration: 0.495s\n",
      "Rolling Lifetime length: 47.190\n",
      "Best Lifetime Rolling Avg: 48.3\n",
      "Best Episode: 2215\n",
      "Time Since Best: 34\n",
      "Has Succeeded: False\n",
      "Stopped Improving: False\n",
      "Metrics: loss: 0.107077, mean_q: 1.214122, mean_eps: 0.773904\n",
      "Total Training Time: 743.999s\n",
      "\n",
      "-----------------\n",
      "                \n",
      "Episode: 2500\n",
      "Step: 25863/1000000\n",
      "This Episode Steps: 4\n",
      "This Episode Reward: 0.0\n",
      "This Episode Duration: 0.314s\n",
      "Rolling Lifetime length: 45.440\n",
      "Best Lifetime Rolling Avg: 48.3\n",
      "Best Episode: 2215\n",
      "Time Since Best: 284\n",
      "Has Succeeded: False\n",
      "Stopped Improving: False\n",
      "Metrics: loss: 0.120247, mean_q: 1.285585, mean_eps: 0.746567\n",
      "Total Training Time: 914.206s\n",
      "\n",
      "-----------------\n",
      "                \n",
      "Episode: 2750\n",
      "Step: 28764/1000000\n",
      "This Episode Steps: 9\n",
      "This Episode Reward: 1.0\n",
      "This Episode Duration: 0.740s\n",
      "Rolling Lifetime length: 47.370\n",
      "Best Lifetime Rolling Avg: 48.82\n",
      "Best Episode: 2680\n",
      "Time Since Best: 69\n",
      "Has Succeeded: False\n",
      "Stopped Improving: False\n",
      "Metrics: loss: 0.121644, mean_q: 1.413049, mean_eps: 0.718162\n",
      "Total Training Time: 1112.196s\n",
      "\n",
      "-----------------\n",
      "                \n",
      "Episode: 3000\n",
      "Step: 31988/1000000\n",
      "This Episode Steps: 14\n",
      "This Episode Reward: 0.0\n",
      "This Episode Duration: 1.295s\n",
      "Rolling Lifetime length: 51.010\n",
      "Best Lifetime Rolling Avg: 51.01\n",
      "Best Episode: 2999\n",
      "Time Since Best: 0\n",
      "Has Succeeded: False\n",
      "Stopped Improving: False\n",
      "Metrics: loss: 0.132164, mean_q: 1.676726, mean_eps: 0.686591\n",
      "Total Training Time: 1359.071s\n",
      "\n",
      "-----------------\n",
      "                \n",
      "Episode: 3250\n",
      "Step: 35188/1000000\n",
      "This Episode Steps: 5\n",
      "This Episode Reward: 0.0\n",
      "This Episode Duration: 0.503s\n",
      "Rolling Lifetime length: 55.340\n",
      "Best Lifetime Rolling Avg: 55.45\n",
      "Best Episode: 3247\n",
      "Time Since Best: 2\n",
      "Has Succeeded: False\n",
      "Stopped Improving: False\n",
      "Metrics: loss: 0.141478, mean_q: 1.803687, mean_eps: 0.655187\n",
      "Total Training Time: 1622.204s\n",
      "\n",
      "-----------------\n",
      "                \n",
      "Episode: 3500\n",
      "Step: 38399/1000000\n",
      "This Episode Steps: 64\n",
      "This Episode Reward: 14.0\n",
      "This Episode Duration: 6.123s\n",
      "Rolling Lifetime length: 56.810\n",
      "Best Lifetime Rolling Avg: 56.91\n",
      "Best Episode: 3494\n",
      "Time Since Best: 5\n",
      "Has Succeeded: False\n",
      "Stopped Improving: False\n",
      "Metrics: loss: 0.175884, mean_q: 1.894778, mean_eps: 0.624008\n",
      "Total Training Time: 1918.309s\n",
      "\n",
      "-----------------\n",
      "                \n",
      "Episode: 3750\n",
      "Step: 42378/1000000\n",
      "This Episode Steps: 5\n",
      "This Episode Reward: 1.0\n",
      "This Episode Duration: 0.606s\n",
      "Rolling Lifetime length: 62.060\n",
      "Best Lifetime Rolling Avg: 62.1\n",
      "Best Episode: 3748\n",
      "Time Since Best: 1\n",
      "Has Succeeded: False\n",
      "Stopped Improving: False\n",
      "Metrics: loss: 0.239576, mean_q: 2.058657, mean_eps: 0.584725\n",
      "Total Training Time: 2333.355s\n",
      "\n",
      "-----------------\n",
      "                \n",
      "Episode: 4000\n",
      "Step: 46140/1000000\n",
      "This Episode Steps: 8\n",
      "This Episode Reward: 2.0\n",
      "This Episode Duration: 0.992s\n",
      "Rolling Lifetime length: 68.010\n",
      "Best Lifetime Rolling Avg: 68.42\n",
      "Best Episode: 3998\n",
      "Time Since Best: 1\n",
      "Has Succeeded: False\n",
      "Stopped Improving: False\n",
      "Metrics: loss: 0.215903, mean_q: 2.323727, mean_eps: 0.547872\n",
      "Total Training Time: 2776.415s\n",
      "\n",
      "-----------------\n",
      "                \n",
      "Episode: 4250\n",
      "Step: 50127/1000000\n",
      "This Episode Steps: 13\n",
      "This Episode Reward: 4.0\n",
      "This Episode Duration: 1.508s\n",
      "Rolling Lifetime length: 71.600\n",
      "Best Lifetime Rolling Avg: 71.6\n",
      "Best Episode: 4249\n",
      "Time Since Best: 0\n",
      "Has Succeeded: False\n",
      "Stopped Improving: False\n",
      "Metrics: loss: 0.254681, mean_q: 2.634367, mean_eps: 0.508824\n",
      "Total Training Time: 3230.079s\n",
      "\n",
      "-----------------\n",
      "                \n",
      "Episode: 4500\n",
      "Step: 54990/1000000\n",
      "This Episode Steps: 6\n",
      "This Episode Reward: 0.0\n",
      "This Episode Duration: 0.747s\n",
      "Rolling Lifetime length: 80.030\n",
      "Best Lifetime Rolling Avg: 80.75\n",
      "Best Episode: 4489\n",
      "Time Since Best: 10\n",
      "Has Succeeded: False\n",
      "Stopped Improving: False\n",
      "Metrics: loss: 0.260832, mean_q: 2.768668, mean_eps: 0.461132\n",
      "Total Training Time: 3794.876s\n",
      "\n",
      "-----------------\n",
      "                \n",
      "Episode: 4750\n",
      "Step: 60064/1000000\n",
      "This Episode Steps: 27\n",
      "This Episode Reward: 5.0\n",
      "This Episode Duration: 3.614s\n",
      "Rolling Lifetime length: 88.300\n",
      "Best Lifetime Rolling Avg: 88.33\n",
      "Best Episode: 4737\n",
      "Time Since Best: 12\n",
      "Has Succeeded: False\n",
      "Stopped Improving: False\n",
      "Metrics: loss: 0.337862, mean_q: 3.139354, mean_eps: 0.411510\n",
      "Total Training Time: 4381.088s\n",
      "\n",
      "-----------------\n",
      "                \n",
      "Episode: 5000\n",
      "Step: 66328/1000000\n",
      "This Episode Steps: 2\n",
      "This Episode Reward: 0.0\n",
      "This Episode Duration: 0.337s\n",
      "Rolling Lifetime length: 102.150\n",
      "Best Lifetime Rolling Avg: 102.17\n",
      "Best Episode: 4998\n",
      "Time Since Best: 1\n",
      "Has Succeeded: False\n",
      "Stopped Improving: False\n",
      "Metrics: loss: 0.390855, mean_q: 3.460305, mean_eps: 0.350000\n",
      "Total Training Time: 5076.367s\n",
      "\n",
      "-----------------\n",
      "                \n",
      "Episode: 5250\n",
      "Step: 74488/1000000\n",
      "This Episode Steps: 70\n",
      "This Episode Reward: 33.0\n",
      "This Episode Duration: 7.085s\n",
      "Rolling Lifetime length: 130.810\n",
      "Best Lifetime Rolling Avg: 131.18\n",
      "Best Episode: 5245\n",
      "Time Since Best: 4\n",
      "Has Succeeded: False\n",
      "Stopped Improving: False\n",
      "Metrics: loss: 0.400330, mean_q: 3.920500, mean_eps: 0.270366\n",
      "Total Training Time: 5924.576s\n",
      "\n",
      "-----------------\n",
      "                \n",
      "Episode: 5500\n",
      "Step: 86320/1000000\n",
      "This Episode Steps: 157\n",
      "This Episode Reward: 66.0\n",
      "This Episode Duration: 14.878s\n",
      "Rolling Lifetime length: 190.820\n",
      "Best Lifetime Rolling Avg: 190.82\n",
      "Best Episode: 5499\n",
      "Time Since Best: 0\n",
      "Has Succeeded: False\n",
      "Stopped Improving: False\n",
      "Metrics: loss: 0.555284, mean_q: 5.056300, mean_eps: 0.154838\n",
      "Total Training Time: 7039.653s\n",
      "\n",
      "-----------------\n",
      "                \n",
      "Episode: 5750\n",
      "Step: 107693/1000000\n",
      "This Episode Steps: 45\n",
      "This Episode Reward: 26.0\n",
      "This Episode Duration: 2.999s\n",
      "Rolling Lifetime length: 384.630\n",
      "Best Lifetime Rolling Avg: 384.7\n",
      "Best Episode: 5748\n",
      "Time Since Best: 1\n",
      "Has Succeeded: False\n",
      "Stopped Improving: False\n",
      "Metrics: loss: 0.661303, mean_q: 6.840440, mean_eps: 0.020000\n",
      "Total Training Time: 8826.514s\n",
      "\n",
      "-----------------\n",
      "                \n",
      "Episode: 6000\n",
      "Step: 143890/1000000\n",
      "This Episode Steps: 303\n",
      "This Episode Reward: 169.0\n",
      "This Episode Duration: 21.220s\n",
      "Rolling Lifetime length: 1096.430\n",
      "Best Lifetime Rolling Avg: 1096.43\n",
      "Best Episode: 5999\n",
      "Time Since Best: 0\n",
      "Has Succeeded: False\n",
      "Stopped Improving: False\n",
      "Metrics: loss: 0.994466, mean_q: 10.671191, mean_eps: 0.020000\n",
      "Total Training Time: 11742.670s\n",
      "\n",
      "-----------------\n",
      "                \n",
      "Episode: 6250\n",
      "Step: 230806/1000000\n",
      "This Episode Steps: 184\n",
      "This Episode Reward: 154.0\n",
      "This Episode Duration: 8.327s\n",
      "Rolling Lifetime length: 2975.230\n",
      "Best Lifetime Rolling Avg: 2975.23\n",
      "Best Episode: 6249\n",
      "Time Since Best: 0\n",
      "Has Succeeded: False\n",
      "Stopped Improving: False\n",
      "Metrics: loss: 2.769813, mean_q: 20.682190, mean_eps: 0.020000\n",
      "Total Training Time: 17225.347s\n",
      "\n",
      "-----------------\n",
      "                \n",
      "Episode: 6500\n",
      "Step: 318746/1000000\n",
      "This Episode Steps: 528\n",
      "This Episode Reward: 374.0\n",
      "This Episode Duration: 23.876s\n",
      "Rolling Lifetime length: 4320.700\n",
      "Best Lifetime Rolling Avg: 4320.7\n",
      "Best Episode: 6499\n",
      "Time Since Best: 0\n",
      "Has Succeeded: False\n",
      "Stopped Improving: False\n",
      "Metrics: loss: 4.276938, mean_q: 26.434053, mean_eps: 0.020000\n",
      "Total Training Time: 21558.111s\n",
      "\n",
      "-----------------\n",
      "                \n",
      "Episode: 6750\n",
      "Step: 426645/1000000\n",
      "This Episode Steps: 1427\n",
      "This Episode Reward: 1176.0\n",
      "This Episode Duration: 63.290s\n",
      "Rolling Lifetime length: 4753.350\n",
      "Best Lifetime Rolling Avg: 4778.72\n",
      "Best Episode: 6744\n",
      "Time Since Best: 5\n",
      "Has Succeeded: False\n",
      "Stopped Improving: False\n",
      "Metrics: loss: 5.897892, mean_q: 32.839067, mean_eps: 0.020000\n",
      "Total Training Time: 26308.733s\n",
      "\n",
      "-----------------\n",
      "                \n",
      "Episode: 7000\n",
      "Step: 544715/1000000\n",
      "This Episode Steps: 332\n",
      "This Episode Reward: 256.0\n",
      "This Episode Duration: 14.068s\n",
      "Rolling Lifetime length: 5401.700\n",
      "Best Lifetime Rolling Avg: 5446.03\n",
      "Best Episode: 6992\n",
      "Time Since Best: 7\n",
      "Has Succeeded: False\n",
      "Stopped Improving: False\n",
      "Metrics: loss: 8.644884, mean_q: 38.963833, mean_eps: 0.020000\n",
      "Total Training Time: 31390.988s\n",
      "\n",
      "-----------------\n",
      "                \n",
      "Episode: 7250\n",
      "Step: 654006/1000000\n",
      "This Episode Steps: 3112\n",
      "This Episode Reward: 2472.0\n",
      "This Episode Duration: 145.967s\n",
      "Rolling Lifetime length: 5405.050\n",
      "Best Lifetime Rolling Avg: 5645.67\n",
      "Best Episode: 7110\n",
      "Time Since Best: 139\n",
      "Has Succeeded: False\n",
      "Stopped Improving: False\n",
      "Metrics: loss: 8.545452, mean_q: 40.028843, mean_eps: 0.020000\n",
      "Total Training Time: 36182.847s\n",
      "\n",
      "-----------------\n",
      "                \n",
      "Episode: 7500\n",
      "Step: 794277/1000000\n",
      "This Episode Steps: 349\n",
      "This Episode Reward: 268.0\n",
      "This Episode Duration: 15.158s\n",
      "Rolling Lifetime length: 5914.230\n",
      "Best Lifetime Rolling Avg: 6058.68\n",
      "Best Episode: 7456\n",
      "Time Since Best: 43\n",
      "Has Succeeded: False\n",
      "Stopped Improving: False\n",
      "Metrics: loss: 8.052691, mean_q: 43.510303, mean_eps: 0.020000\n",
      "Total Training Time: 42090.798s\n",
      "\n",
      "-----------------\n",
      "                \n",
      "Episode: 7750\n",
      "Step: 949408/1000000\n",
      "This Episode Steps: 943\n",
      "This Episode Reward: 764.0\n",
      "This Episode Duration: 39.328s\n",
      "Rolling Lifetime length: 7052.050\n",
      "Best Lifetime Rolling Avg: 7105.62\n",
      "Best Episode: 7748\n",
      "Time Since Best: 1\n",
      "Has Succeeded: False\n",
      "Stopped Improving: False\n",
      "Metrics: loss: 7.725942, mean_q: 44.121722, mean_eps: 0.020000\n",
      "Total Training Time: 48570.639s\n",
      "\n",
      "Training Finished in 50665.344 seconds\n",
      "        \n",
      "Final Step: 1000000\n",
      "Succeeded: False\n",
      "Stopped_Improving: False\n",
      "Final Episode Lifetimes Rolling Avg: 7544.970\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now()\n",
    "started_file = os.path.join(logging_directory,\"started_at.p\")\n",
    "pickle.dump(now, open(started_file, \"wb\" ) )\n",
    "\n",
    "history = dqn.fit(env, \n",
    "  nb_steps=all_configs[\"max_timesteps\"], \n",
    "  action_repetition=1, \n",
    "  callbacks=[logging_callback], \n",
    "  verbose=2,\n",
    "  visualize=False, \n",
    "  nb_max_start_steps=0, \n",
    "  start_step_policy=None, \n",
    "  log_interval=all_configs[\"print_freq\"],\n",
    "  nb_max_episode_steps=None, \n",
    "  episode_averaging_length=all_configs[\"rolling_average_length\"], \n",
    "  success_threshold=all_configs[\"success_threshold\"],\n",
    "  stopping_patience=all_configs[\"stopping_patience\"],\n",
    "  min_nb_steps=all_configs[\"exploration_fraction\"],\n",
    "  single_cycle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, during the training procedure various statistics are logged, both to stdout and to file in the specified directory. As you may notice above we manually stopped training after approximately 7000 seconds while the agent was still improving, and before it has reached the specified success threshold.\n",
    "\n",
    "In order to evaluate the agent later on, or apply the agent in a production decoding scenario we can easily save the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_file = os.path.join(logging_directory, \"dqn_weights_0.7.h5f\")\n",
    "dqn.save_weights(weights_file, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, in order to evaluate the training procedure we may be interested in viewing any of the metrics which were logged. These are all saved within the history.history dictionary. For example, we are often most interested in analyzing the training procedure by looking at the rolling average of the qubit lifetime, which we can do as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "training_history = history.history[\"episode_lifetimes_rolling_avg\"]\n",
    "\n",
    "plt.figure(figsize=(12,7))\n",
    "plt.plot(training_history)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Rolling Average Qubit Lifetime')\n",
    "_ = plt.title(\"Training History\")\n",
    "plt.show()\n",
    "plt.savefig(\"training_history_per_alpha0_7.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot one can see that during the exploration phase the agent was unable to do well, due to constant exploratory random actions, but was able to exploit this knowledge effectively once the exploration probability became sufficiently low. Again, it is also clear that the agent was definitely still learning and improving when we chose to stop the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join(os.getcwd(),\"results_0_7\")\n",
    "\n",
    "model = build_convolutional_nn(all_configs[\"c_layers\"],all_configs[\"ff_layers\"], env.observation_space.shape, env.num_actions)\n",
    "memory = SequentialMemory(limit=all_configs[\"buffer_size\"], window_length=1, enable_prioritized_replay=True)\n",
    "policy = GreedyQPolicy(masked_greedy=True)\n",
    "test_policy = GreedyQPolicy(masked_greedy=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "dqn = DQNAgent(model=model, \n",
    "               nb_actions=env.num_actions, \n",
    "               memory=memory, \n",
    "               nb_steps_warmup=all_configs[\"learning_starts\"], \n",
    "               target_model_update=all_configs[\"target_network_update_freq\"], \n",
    "               policy=policy,\n",
    "               test_policy=test_policy,\n",
    "               gamma = all_configs[\"gamma\"],\n",
    "               enable_dueling_network=all_configs[\"dueling\"],\n",
    "               enable_prioritized_replay=True)\n",
    "\n",
    "\n",
    "dqn.compile(Adam(lr=all_configs[\"learning_rate\"]))\n",
    "dqn.model.load_weights(weights_file)\n",
    "\n",
    "# -------------------------------------------------------------------------------------------\n",
    "\n",
    "trained_at = all_configs[\"p_phys\"]\n",
    "num_to_test = 20\n",
    "error_rates = [j*0.001 for j in range(1,num_to_test + 1)]\n",
    "thresholds = [1/p for p in error_rates]\n",
    "nb_test_episodes = all_configs[\"testing_length\"]\n",
    "all_results = {}\n",
    "\n",
    "\n",
    "keep_evaluating = True\n",
    "count = 0\n",
    "while keep_evaluating:\n",
    "\n",
    "  err_rate = error_rates[count]\n",
    "  env.p_phys = err_rate\n",
    "  env.p_meas = err_rate\n",
    "\n",
    "  dict_key = str(err_rate)[:5]\n",
    "\n",
    "\n",
    "  testing_history = dqn.test(env,nb_episodes = nb_test_episodes, visualize=False, verbose=2, interval=10, single_cycle=False)\n",
    "  results = testing_history.history[\"episode_lifetimes_rolling_avg\"]\n",
    "  final_result = results[-1:][0]\n",
    "  all_results[dict_key] = final_result\n",
    "\n",
    "  if abs(trained_at - err_rate) < 1e-6:\n",
    "    results_file = os.path.join(output_dir,\"results.p\")\n",
    "    pickle.dump(results, open(results_file, \"wb\" ))\n",
    "\n",
    "  to_beat = thresholds[count]\n",
    "  if count == (num_to_test - 1):\n",
    "    keep_evaluating = False\n",
    "\n",
    "  count += 1\n",
    "\n",
    "all_results_file = os.path.join(output_dir,\"all_results.p\")\n",
    "pickle.dump(all_results, open(all_results_file, \"wb\" ))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "threepointsix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "34c3dbb8c5a0679194bd98687bbba2e6013b761a46fa6aa4d88115afd8aaf919"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
